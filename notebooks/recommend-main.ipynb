{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55106d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick_start_pytorch_images  refpred\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fcd9b2b-a2e7-4590-af14-01cbff6c06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c pytorch faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e4ff16-2fa7-4b70-a069-11d574edeccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KDTree, BallTree\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "import ast\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9fda176-2706-4bb1-83f6-f924aa7bb1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 0, 'Quadro RTX 5000')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.cuda.current_device(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0511d484-fb64-46b0-899a-a2215f99b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'refpred/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8c84642-630d-4fd7-81d9-1e7d060bef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DATA_PATH}/output_10k.json', 'r') as f:\n",
    "    embeddings = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "543c4477",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DATA_PATH}/metadata_10k_full.json', 'r') as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6574b5aa-d74d-44ba-a313-44241a1dd7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9319"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# originally there were 10K papers, however removing papers with null title/abstract or 0 references resulted in 700 less\n",
    "all_paper_ids = [e['paper_id'] for e in embeddings]\n",
    "zero_ref_pids = [pid for pid in all_paper_ids if len(ast.literal_eval(metadata[pid]['references'])) == 0]\n",
    "all_paper_ids = list(set(all_paper_ids) - set(zero_ref_pids))\n",
    "all_pid_set = set(all_paper_ids)\n",
    "len(all_paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48bd6e4f-a1c9-4a06-847b-b6e20bd65bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 0 reference pids\n",
    "embeddings = [e for e in embeddings if e['paper_id'] in all_pid_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "626cfee4-7f37-4861-8e31-8bda45a0a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {k:v for k,v in metadata.items() if k in all_pid_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "132bdd4f-a585-414b-822d-b1a44b7dd551",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in metadata:\n",
    "    p = metadata[pid]\n",
    "    p['references'] = ast.literal_eval(p['references'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a1097eb-6dbb-496d-aa75-9d8c89a1b171",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Maybe a list of all references needed if we are preparing reranking model data randomly and\n",
    "# # need to compare years of the references to the query paper (to ensure the model does not have \n",
    "# # access to future information \n",
    "\n",
    "\n",
    "# for pid in metadata:\n",
    "#     p = metadata[pid]\n",
    "#     p['references'] = ast.literal_eval(p['references'])\n",
    "\n",
    "# all_references = set()\n",
    "# for pid in metadata:\n",
    "#     p = metadata[pid]\n",
    "#     p_refs = p['references']\n",
    "#     all_references.update(p_refs)\n",
    "# len(all_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa8d2e98-ab32-47cb-a9c6-035dbac955d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.stack([torch.tensor(e['embedding']) for e in embeddings]).double()\n",
    "# all_paper_ids = [e['paper_id'] for e in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f2149a9-0315-4464-8bb1-da6c0970bb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9319"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2742c571-b218-4637-bfd1-43b6c3763669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9319, 9319)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings), len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f2367e2-7993-430d-8671-1e64155184c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'659408b243cec55de8d0a3bc51b81173007aa89b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/siddharth/github/refpred/notebooks/recommend-main.ipynb Cell 16\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/siddharth/github/refpred/notebooks/recommend-main.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m metadata[\u001b[39m'\u001b[39;49m\u001b[39m659408b243cec55de8d0a3bc51b81173007aa89b\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mreferences\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: '659408b243cec55de8d0a3bc51b81173007aa89b'"
     ]
    }
   ],
   "source": [
    "metadata['659408b243cec55de8d0a3bc51b81173007aa89b']['references']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8ada4f4-2e14-4022-a363-53065953572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map = {}\n",
    "for obj in embeddings:\n",
    "    # object is a dict like {'paper_id': str, 'embedding': np.array}\n",
    "    paper_id, emb_768 = obj.values()\n",
    "    arr = np.asarray(emb_768)\n",
    "    embedding_map[paper_id] = torch.tensor(arr, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63ca48fe-d3b1-461f-9c68-977c9fa5c98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9319"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d757f70c-8aa7-46b6-961a-dc45a87fa0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.array([embedding_map[paper_id].numpy() for paper_id in all_paper_ids])\n",
    "\n",
    "# We can use l2 distance ªas cosine distance isn't really a thing in sklearn BallTree/KDTree) so long as the input matrix is l2 normalized\n",
    "# in this case it will result in the same ordering as a cosine distance\n",
    "# https://stackoverflow.com/questions/34144632/using-cosine-distance-with-scikit-learn-kneighborsclassifier\n",
    "knn_tree = BallTree(embedding_matrix, metric='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5d607bf-a859-489f-83af-674bcb0ec381",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_map = {}\n",
    "# fetch references from metadata dict in the format {paper_id: [list of references]}\n",
    "for paper_id in all_paper_ids:\n",
    "    # references = ast.literal_eval(metadata[paper_id].get('references'))\n",
    "    references = metadata[paper_id].get('references')\n",
    "    reference_map[paper_id] = references or []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e17981d9-9969-44b0-bb5f-1d3355b263bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9319"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reference_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b27c918-511f-4dd5-afa5-99677ff32d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paper_ids, test_paper_ids = train_test_split(all_paper_ids, test_size=0.2, random_state=42)\n",
    "val_paper_ids, test_paper_ids = train_test_split(test_paper_ids, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12e8e416-c0c9-4ee6-9962-1c76e9dc4b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9319"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_paper_ids) +  len(val_paper_ids) + len(test_paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5428d18-cd17-46d9-a575-31282e8f2264",
   "metadata": {},
   "outputs": [],
   "source": [
    "specter_tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "specter_model = AutoModel.from_pretrained('allenai/specter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63ed90a8-62e2-443d-ba5a-783037602bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_embedding(paper, embedding_map=None):\n",
    "    '''\n",
    "    Given an input paper (dict with at least 'title' as a key, returns a 768 dimensional embeddings using the SPECTER model from HF.\n",
    "    '''\n",
    "    assert type(paper)==dict and 'title' in paper.keys(), \"paper must be a dict with at least 'title' as a key\"\n",
    "    if embedding_map and paper['paper_id'] is not None and paper['paper_id'] in embedding_map:\n",
    "        return embedding_map[paper['paper_id']].view(1,-1)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "    # model = AutoModel.from_pretrained('allenai/specter')\n",
    "    title_abs = [d['title'] + specter_tokenizer.sep_token + (d.get('abstract') or '') for d in [paper]]\n",
    "    inputs = specter_tokenizer(title_abs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    result = specter_model(**inputs)\n",
    "    cur_embedding = result.last_hidden_state[:, 0, :]\n",
    "    return cur_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b517e19b-dc72-4402-b9be-ae157a923fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_knn(cur_embedding, knn_tree, all_paper_ids, k=20, least = False):\n",
    "    # current_embeddings shape: [1,768]\n",
    "    # assert(torch.is_tensor(cur_embedding))\n",
    "    # print(cur_embedding.shape)\n",
    "    # assert(cur_embedding.shape==(1,768))\n",
    "    scores, top_indices = knn_tree.query(cur_embedding.reshape(1,-1), k=k, return_distance=True)\n",
    "    scores, top_indices = np.squeeze(scores), np.squeeze(top_indices)\n",
    "    return top_indices, scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6196973-da60-499b-adad-8787ae7176d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_knn_pid(pid, knn_tree, embedding_map, metadata, all_paper_ids, k=20, least = False):\n",
    "    paper_embedding = get_embedding(metadata[pid], embedding_map)\n",
    "    top_indices, scores = find_knn(paper_embedding, knn_tree, all_paper_ids, k=20, least = False)\n",
    "    recommended_paper_ids = [all_paper_ids[i] for i in top_indices]\n",
    "    return recommended_paper_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f72b46b0-6356-4081-a555-5d9291c26fcd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## V1\n",
    "# @torch.no_grad()\n",
    "# def find_similar_knn(cur_embedding, embedding_map, k=10, least = False):\n",
    "#     # current_embeddings shape: [1,768]\n",
    "#     # weights shape: [num_of_egs, embedding_dim=768]\n",
    "#     # cur_embedding = get_embedding(paper)\n",
    "#     # input shape should be [1,768]\n",
    "#     weights = \n",
    "#     assert(torch.is_tensor(cur_embedding))\n",
    "#     # print(cur_embedding.shape)\n",
    "#     assert(cur_embedding.shape==(1,768))\n",
    "#     weights_norm = F.normalize(weights, p=2, dim=1).double() # (N, d)\n",
    "#     cur_em_norm = F.normalize(cur_embedding, p=2, dim=1).double() # (1, d)\n",
    "#     cos_sim = F.cosine_similarity(weights_norm, cur_em_norm, dim=1)\n",
    "#     topk = torch.topk(cos_sim, k, largest = False if least else True)\n",
    "#     top_indices = topk.indices\n",
    "#     top_values = topk.values\n",
    "#     return top_indices, top_values\n",
    "\n",
    "## V2\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def find_similar_knn(cur_embedding, weights, k=10, least = False):\n",
    "#     # current_embeddings shape: [1,768]\n",
    "#     # weights shape: [num_of_egs, embedding_dim=768]\n",
    "#     # cur_embedding = get_embedding(paper)\n",
    "#     # input shape should be [1,768]\n",
    "#     assert(torch.is_tensor(cur_embedding))\n",
    "#     # print(cur_embedding.shape)\n",
    "#     assert(cur_embedding.shape==(1,768))\n",
    "#     weights_norm = F.normalize(weights, p=2, dim=1).double() # (N, d)\n",
    "#     cur_em_norm = F.normalize(cur_embedding, p=2, dim=1).double() # (1, d)\n",
    "#     cos_sim = F.cosine_similarity(weights_norm, cur_em_norm, dim=1)\n",
    "#     topk = torch.topk(cos_sim, k, largest = False if least else True)\n",
    "#     top_indices = topk.indices\n",
    "#     top_values = topk.values\n",
    "#     return top_indices, top_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b64acb75-59e7-4646-bed5-b0566d426ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(recommended, actual, k=None):\n",
    "    recommended = np.asarray(recommended)[:k] if k else np.asarray(recommended)\n",
    "    actual = np.asarray(actual)\n",
    "    \n",
    "    if len(actual) == 0:\n",
    "        print('No actual references stored in the metadata')\n",
    "        return 0,0,0\n",
    "    \n",
    "    true_positives = np.intersect1d(recommended, actual)\n",
    "    false_positives = np.setdiff1d(recommended, actual)\n",
    "    false_negatives = np.setdiff1d(actual, recommended)\n",
    "    \n",
    "    assert(len(true_positives) + len(false_positives) == len(recommended))\n",
    "    assert(len(true_positives) + len(false_negatives) == len(actual))\n",
    "\n",
    "    precision = len(true_positives) / (len(true_positives) + len(false_positives))\n",
    "    recall = len(true_positives) / (len(true_positives) + len(false_negatives))\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    \n",
    "    return precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f3b93ef8-49ed-44f0-a37b-a43e9066476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a3e4ceb42cbcd2c807d53aff90a8cb1f5ee3f031\n",
    "new_title = 'SPECTER: Document-level Representation Learning using Citation-informed Transformers'\n",
    "\n",
    "new_abstract = 'We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6aff73e4-3d7a-42d9-893a-d4fd2aca3be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 204e3073870fae3d05bcbc2f6a8e263d9b72e776\n",
    "new_title = 'Attention Is All You Need'\n",
    "new_abstract = 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "de0b5b8a-3f02-41f5-a828-96d96177499b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### MUCH MUCH FASTER ALGORITHM PRESENT BELOW\n",
    "# # new_paper = {'title': new_title, 'abstract': new_abstract or ''}\n",
    "# # assert new_paper['title'] is not None\n",
    "# # paper_id = '204e3073870fae3d05bcbc2f6a8e263d9b72e776'\n",
    "# k = 20\n",
    "# test_paper_id = '204e3073870fae3d05bcbc2f6a8e263d9b72e776'\n",
    "# paper_embedding = get_embedding(metadata[test_paper_id])\n",
    "# top_indices, top_values = find_similar_knn(paper_embedding, weights, k=50, least=False)\n",
    "# recommended_paper_ids = [all_paper_ids[i] for i in top_indices]\n",
    "\n",
    "\n",
    "# cnt = 0\n",
    "# for paper_id, cos_sim in zip(recommended_paper_ids, top_values):\n",
    "#     title = metadata[paper_id]['title']\n",
    "#     # abstract = metadata[paper_id]['abstract']\n",
    "#     year = metadata[paper_id]['year']\n",
    "#     print(f'Paper ID: {paper_id}\\nTitle: {title}\\nYear: {year}')# \\nCosine similarity: {cos_sim}\\n')\n",
    "#     cnt += 1 \n",
    "#     if cnt == 10:\n",
    "#         break\n",
    "        \n",
    "# actual_references = ast.literal_eval(metadata[test_paper_id].get('references'))\n",
    "\n",
    "# print('-'*100)\n",
    "\n",
    "# precision, recall, f1_score = evaluate(recommended_paper_ids[1:], actual_references, k=k)\n",
    "# print(f'For the Paper - {metadata[test_paper_id][\"title\"]}')\n",
    "# print(f\"Precision @ {k}: {precision}\")\n",
    "# print(f\"Recall @ {k}: {recall}\")\n",
    "# print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f01918a8-dde2-4ab0-8626-a71afa423123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper ID: 204e3073870fae3d05bcbc2f6a8e263d9b72e776\n",
      "Title: Attention is All you Need\n",
      "Year: 2017\n",
      "Paper ID: 9ae0a24f0928cab1554a6ac880f6b350f85be698\n",
      "Title: One Model To Learn Them All\n",
      "Year: 2017\n",
      "Paper ID: b60abe57bc195616063be10638c6437358c81d1e\n",
      "Title: Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation\n",
      "Year: 2016\n",
      "Paper ID: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd\n",
      "Title: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\n",
      "Year: 2016\n",
      "Paper ID: 43428880d75b3a14257c3ee9bda054e61eb869c0\n",
      "Title: Convolutional Sequence to Sequence Learning\n",
      "Year: 2017\n",
      "Paper ID: 93499a7c7f699b6630a86fad964536f9423bb6d0\n",
      "Title: Effective Approaches to Attention-based Neural Machine Translation\n",
      "Year: 2015\n",
      "Paper ID: 4550a4c714920ef57d19878e31c9ebae37b049b2\n",
      "Title: Massive Exploration of Neural Machine Translation Architectures\n",
      "Year: 2017\n",
      "Paper ID: 2f2d8f8072e5cc9b296fad551f65f183bdbff7aa\n",
      "Title: Exploring the Limits of Language Modeling\n",
      "Year: 2016\n",
      "Paper ID: bb669de2fce407df2f5cb2f8c51dedee3f467e04\n",
      "Title: The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation\n",
      "Year: 2018\n",
      "Paper ID: 25eb839f39507fe6983ad3e692b2f8d93a5cb0cc\n",
      "Title: Montreal Neural Machine Translation Systems for WMT’15\n",
      "Year: 2015\n",
      "Paper ID: 247328a082d86199ed5a98e1d726aa205c1da9df\n",
      "Title: Neural Machine Translation\n",
      "Year: 2017\n",
      "----------------------------------------------------------------------------------------------------\n",
      "For the Paper - Attention is All you Need\n",
      "Precision @ 20: 0.3684210526315789\n",
      "Recall @ 20: 0.1891891891891892\n",
      "F1 Score: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Cosine scores not available because BallTree doesn't have a cosine similarity metric however similar ranking is providied by normalized L2\n",
    "\n",
    "k = 20\n",
    "test_paper_id = '204e3073870fae3d05bcbc2f6a8e263d9b72e776'\n",
    "# test_paper_id = '09ebd9ad4fa21c0d56433ac57a4cd69e94c72281'\n",
    "# test_paper = metadata[test_paper_id]\n",
    "# print(test_paper)\n",
    "recommended_paper_ids = find_knn_pid(test_paper_id, knn_tree, embedding_map, metadata, all_paper_ids)\n",
    "\n",
    "\n",
    "for i, pid in enumerate(recommended_paper_ids):\n",
    "    title = metadata[pid]['title']\n",
    "    # abstract = metadata[paper_id]['abstract']\n",
    "    year = metadata[pid]['year']\n",
    "    print(f'Paper ID: {pid}\\nTitle: {title}\\nYear: {year}')# \\nCosine similarity: {cos_sim}\\n')\n",
    "    if i == 10:\n",
    "        break\n",
    "\n",
    "# NOT NEEDED as this is already done during metadata init\n",
    "# actual_references = ast.literal_eval(metadata[test_paper_id].get('references'))\n",
    "actual_references = metadata[test_paper_id].get('references')\n",
    "\n",
    "print('-'*100)\n",
    "\n",
    "precision, recall, f1_score = evaluate(recommended_paper_ids[1:], actual_references, k=k)\n",
    "print(f'For the Paper - {metadata[test_paper_id][\"title\"]}')\n",
    "print(f\"Precision @ {k}: {precision}\")\n",
    "print(f\"Recall @ {k}: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fcfd916-57e0-4fe0-a4d3-d6080378c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_knn(test_paper_ids, knn_tree, embedding_map, metadata, reference_map, all_paper_ids, k=20):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for paper_id in tqdm(test_paper_ids):\n",
    "        # paper_embedding = embedding_map[paper_id]\n",
    "        # # Query the KD-tree to find k nearest neighbors\n",
    "        # top_indices = knn_tree.query(paper_embedding.reshape(1,-1), k=k, return_distance=False)[0]\n",
    "        # recommendations = set([all_paper_ids[i] for i in top_indices])\n",
    "        recommendations = set(find_knn_pid(paper_id, knn_tree, embedding_map, metadata, all_paper_ids))\n",
    "        true_references = set(reference_map.get(paper_id, []))\n",
    "        if not true_references:\n",
    "            continue\n",
    "        intersect = recommendations.intersection(true_references)\n",
    "        precision = len(intersect) / len(recommendations)\n",
    "        recall = len(intersect) / len(true_references)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    mean_precision = sum(precisions) / len(precisions)\n",
    "    mean_recall = sum(recalls) / len(recalls)\n",
    "    return mean_precision, mean_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f0e0530-57f7-4f45-b698-cc3a07ba5ea6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def evaluate_knn(test_paper_ids, embedding_map, reference_map, k=20):\n",
    "#     precisions = []\n",
    "#     recalls = []\n",
    "#     for paper_id in tqdm(test_paper_ids):\n",
    "#         paper_embedding = embedding_map[paper_id]\n",
    "#         top_indices, top_values = find_similar_knn(paper_embedding.view(1,-1), weights, k=k, least=False)\n",
    "#         recommendations = set([all_paper_ids[i] for i in top_indices])\n",
    "#         # recommendations = set(get_recommendations(model, paper_id, embedding_map, k=k))\n",
    "#         true_references = set(reference_map.get(paper_id, []))\n",
    "#         if not true_references:\n",
    "#             continue\n",
    "\n",
    "#         intersect = recommendations.intersection(true_references)\n",
    "#         precision = len(intersect) / len(recommendations)\n",
    "#         recall = len(intersect) / len(true_references)\n",
    "\n",
    "#         # precision, recall, f1_score = evaluate(list(recommendations), list(true_references), k=k)\n",
    "#         # print(f'({len(recommendations)},{len(true_references)}){precision=}    {recall=}')\n",
    "\n",
    "#         precisions.append(precision)\n",
    "#         recalls.append(recall)\n",
    "\n",
    "#     mean_precision = sum(precisions) / len(precisions)\n",
    "#     mean_recall = sum(recalls) / len(recalls)\n",
    "#     return mean_precision, mean_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bdc1602-7859-46cd-9cb3-49729e7a1c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 85.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision @ 20: 0.09949999999999999\n",
      "Recall @ 20: 0.08094859861055337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "precision, recall = evaluate_knn(train_paper_ids[:100], knn_tree, embedding_map, metadata, reference_map, all_paper_ids, k=20)\n",
    "print(f\"Precision @ 20: {precision}\")\n",
    "print(f\"Recall @ 20: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea4093ad-87f3-4924-b08e-b072ab9a126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 932/932 [00:10<00:00, 86.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision @ 20: 0.08648068669527863\n",
      "Recall @ 20: 0.06613985349169942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "precision, recall = evaluate_knn(test_paper_ids, knn_tree, embedding_map, metadata, reference_map, all_paper_ids, k=20)\n",
    "print(f\"Precision @ 20: {precision}\")\n",
    "print(f\"Recall @ 20: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30305815-877a-4abf-b904-abea2abaad14",
   "metadata": {},
   "source": [
    "### For test.txt IDs\n",
    "Precision @ P20: 0.08703108252947454\n",
    "Recall @ 20: 0.07151955632733407"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8ac7ad-d67e-4aa3-928a-e59a9350fb28",
   "metadata": {},
   "source": [
    "## NN Based Re-ranking from the KNN candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b46ea388-21b1-4652-9791-ea23cd0fb4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e5d112c-54d6-449f-b7f5-fc673b8e298f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class PaperPairDataset(Dataset):\n",
    "#     def __init__(self, embedding_map, reference_map, all_paper_ids, num_positives=5, num_negatives=5, knn=False, k=20):\n",
    "#         self.embedding_map = embedding_map\n",
    "#         self.reference_map = reference_map\n",
    "#         self.all_paper_ids = all_paper_ids\n",
    "#         self.num_positives = num_positives\n",
    "#         self.num_negatives = num_negatives\n",
    "#         self.k = k\n",
    "#         self.knn = knn\n",
    "#         self.paper_ids = list(embedding_map.keys())\n",
    "#         if self.knn:\n",
    "#             self.prepare_data_knn(k=self.k)\n",
    "#         else:\n",
    "#             self.prepare_data_random()\n",
    "\n",
    "\n",
    "#     def prepare_data_random(self):\n",
    "#         self.data = []\n",
    "#         for paper_id in self.paper_ids:\n",
    "#             pos_count = 0\n",
    "#             if paper_id in self.reference_map:\n",
    "#                 for ref_id in self.reference_map[paper_id]:\n",
    "#                     if ref_id in self.embedding_map:\n",
    "#                         self.data.append((paper_id, ref_id, 1))\n",
    "#                         pos_count += 1\n",
    "#                         if pos_count >= self.num_positives:\n",
    "#                             break\n",
    "\n",
    "#             neg_count = 0\n",
    "#             while neg_count < self.num_negatives:\n",
    "#                 random_id = random.choice(self.paper_ids)\n",
    "#                 if random_id not in self.reference_map.get(paper_id, []) and random_id != paper_id:\n",
    "#                     self.data.append((paper_id, random_id, 0))\n",
    "#                     neg_count += 1\n",
    "                    \n",
    "#     def prepare_data_knn(self, k=20):\n",
    "#         self.data = []\n",
    "#         for i, paper_id in enumerate(self.paper_ids):\n",
    "#             pos_count = 0\n",
    "#             neg_count = 0\n",
    "#             paper_embedding = embedding_map[paper_id]\n",
    "#             top_indices, top_values = find_similar_knn(paper_embedding.view(1,-1), weights, k=k, least=False)\n",
    "#             recommendations = [self.all_paper_ids[i] for i in top_indices]\n",
    "#             for ref_id in recommendations:\n",
    "#                 # a positive example\n",
    "#                 if pos_count >= self.num_positives and neg_count >= self.num_negatives:\n",
    "#                     break\n",
    "#                 if ref_id in self.reference_map[paper_id]:\n",
    "#                     if pos_count >= self.num_positives:\n",
    "#                         continue\n",
    "#                     self.data.append((paper_id, ref_id, 1))\n",
    "#                     pos_count += 1\n",
    "#                 else:\n",
    "#                     if neg_count >= self.num_negatives:\n",
    "#                         continue\n",
    "#                     self.data.append((paper_id, ref_id, 0))\n",
    "#                     neg_count += 1\n",
    "                    \n",
    "#             if i % 100 == 0:\n",
    "#                 print(f'{i} papers processed')\n",
    "\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         paper_id1, paper_id2, score = self.data[idx]\n",
    "#         embedding1 = self.embedding_map[paper_id1]\n",
    "#         embedding2 = self.embedding_map[paper_id2]\n",
    "#         # return torch.tensor(embedding1), torch.tensor(embedding2), torch.tensor(score, dtype=torch.float32)\n",
    "#         return embedding1, embedding2, torch.tensor(score, dtype=torch.float32)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0a7d9a75-cfcf-4db6-a18b-9eb2f225e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperPairDataset(Dataset):\n",
    "    def __init__(self, paper_ids, embedding_map, reference_map, metadata_map, all_paper_ids, num_positives=5, num_negatives=5, use_knn=False, knn_tree = None, k=20, check_year=True):\n",
    "        self.paper_ids = paper_ids\n",
    "        self.embedding_map = embedding_map\n",
    "        self.reference_map = reference_map\n",
    "        self.metadata_map = metadata_map\n",
    "        self.all_paper_ids = all_paper_ids\n",
    "        self.num_positives = num_positives\n",
    "        self.num_negatives = num_negatives\n",
    "        self.use_knn = use_knn\n",
    "        self.k = k\n",
    "        self.knn_tree = knn_tree\n",
    "        self.check_year = check_year\n",
    "        # self.paper_ids = list(embedding_map.keys())\n",
    "        if self.use_knn:\n",
    "            self.prepare_data_knn(k=self.k)\n",
    "        else:\n",
    "            self.prepare_data_random()\n",
    "\n",
    "\n",
    "    def prepare_data_random(self):\n",
    "        # WARNING: if checking years for references, it might lead to KeyError as not all references are present in the metadata dict\n",
    "        # So, handle either with try-catch and continuing OR getting the metadata for all references (using S2 batch API)\n",
    "        self.data = []\n",
    "        for paper_id in self.paper_ids:\n",
    "            original_year = self.metadata_map[paper_id]['year']\n",
    "            assert type(original_year) == int\n",
    "            if paper_id in self.reference_map:\n",
    "                pos_count = 0\n",
    "                for ref_id in self.reference_map[paper_id]:\n",
    "                    # TODO: THESE 2 LINES MAY CAUSE ERROR because ref_id my not be in metadata_map\n",
    "                    reference_year = self.metadata_map[ref_id]['year']\n",
    "                    assert type(reference_year) == int                    \n",
    "                    if reference_year > original_year:\n",
    "                        # skip references that were not present at the time of the paper\n",
    "\n",
    "                        # print(f'Skipping {ref_id} because it was published after {paper_id} ({reference_year} > {original_year})')\n",
    "                        continue\n",
    "\n",
    "                    \n",
    "                    if ref_id in self.embedding_map:\n",
    "                        self.data.append((paper_id, ref_id, 1))\n",
    "                        pos_count += 1\n",
    "                        if pos_count >= self.num_positives:\n",
    "                            break\n",
    "\n",
    "            neg_count = 0\n",
    "            while neg_count < self.num_negatives:\n",
    "                random_id = random.choice(self.paper_ids)\n",
    "                if random_id not in self.reference_map.get(paper_id, []) and random_id != paper_id:\n",
    "                    self.data.append((paper_id, random_id, 0))\n",
    "                    neg_count += 1\n",
    "                    \n",
    "    def prepare_data_knn(self, k):\n",
    "        self.data = []\n",
    "        total_pos = 0\n",
    "        total_neg = 0\n",
    "        zero_ref_papers = 0\n",
    "        for i, paper_id in enumerate(tqdm(self.paper_ids)):\n",
    "            pos_count = 0\n",
    "            neg_count = 0\n",
    "            # paper_embedding = embedding_map[paper_id]\n",
    "            original_year = self.metadata_map[paper_id]['year']\n",
    "            assert type(original_year) == int\n",
    "            # top_indices, top_values = find_similar_knn(paper_embedding.view(1,-1), weights, k=k, least=False)\n",
    "            # recommendations = [self.all_paper_ids[i] for i in top_indices]\n",
    "            \n",
    "            recommendations = set(find_knn_pid(paper_id, self.knn_tree, self.embedding_map, self.metadata_map, self.all_paper_ids))\n",
    "            actual_references = self.reference_map[paper_id]\n",
    "            if len(actual_references) == 0:\n",
    "                zero_ref_papers += 1\n",
    "                \n",
    "            for ref_id in recommendations:\n",
    "                reference_year = self.metadata_map[ref_id]['year']\n",
    "                assert type(reference_year) == int\n",
    "                if self.check_year and reference_year > original_year:\n",
    "                    # skip KNN receommendations that were not present at the time of the paper\n",
    "\n",
    "                    # print(f'Skipping {ref_id} because it was published after {paper_id} ({reference_year} > {original_year})')\n",
    "                    continue\n",
    "\n",
    "                if pos_count >= self.num_positives and neg_count >= self.num_negatives:\n",
    "                    \n",
    "                    break\n",
    "                if ref_id in actual_references and pos_count < self.num_positives:\n",
    "                    # a positive example\n",
    "                    self.data.append((paper_id, ref_id, 1))\n",
    "                    pos_count += 1\n",
    "                elif ref_id not in actual_references and neg_count < self.num_negatives:\n",
    "                    # a negative example\n",
    "                    self.data.append((paper_id, ref_id, 0))\n",
    "                    neg_count += 1\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            # if i % 1000 == 0:\n",
    "            #     print(f'{i} papers processed')\n",
    "            total_pos += pos_count\n",
    "            total_neg += neg_count\n",
    "        print(f'{len(self.data)} pairs added with {total_pos} positive pairs and {total_neg} negative pairs | +/- ratio = {total_pos/total_neg:.2f} | {zero_ref_papers} papers did have references stored in metadata dict.')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        paper_id1, paper_id2, score = self.data[idx]\n",
    "        embedding1 = self.embedding_map[paper_id1]\n",
    "        embedding2 = self.embedding_map[paper_id2]\n",
    "        # return torch.tensor(embedding1), torch.tensor(embedding2), torch.tensor(score, dtype=torch.float32)\n",
    "        return embedding1, embedding2, torch.tensor(score, dtype=torch.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33b44618-b2f5-456a-a263-642424b913dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7937"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e60f771d-c38b-43c6-ae6a-04932baf2872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 80.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 pairs added with 4 positive pairs and 21 negative pairs : +/- ratio = 0.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.PaperPairDataset at 0x7fb18838aa30>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PaperPairDataset(train_paper_ids[:5], embedding_map, reference_map, metadata, all_paper_ids, use_knn=True, k=1000, knn_tree=knn_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "35e6fffa-e4e6-41a4-9dd2-b103d623396b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7937/7937 [01:37<00:00, 81.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51670 pairs added with 11989 positive pairs and 39681 negative pairs | +/- ratio = 0.30 | 489 papers did have references stored in metadata dict.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the dataset, model, loss function, and optimizer\n",
    "train_dataset = PaperPairDataset(train_paper_ids, embedding_map, reference_map, metadata, all_paper_ids, use_knn=True, k=1000, knn_tree=knn_tree, num_positives=5, num_negatives=5, check_year = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f510a3-8ef2-4b0b-b1c9-a4f70dde249a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1627ca0-a834-4fa4-9f77-1ec2bd1fcdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48243"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "128ea240-b114-4477-9a3d-ec44bd3ea614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('09ebd9ad4fa21c0d56433ac57a4cd69e94c72281',\n",
       "  '02c76c01bef98edbd8a2f2041454035f77837ace',\n",
       "  0),\n",
       " ('09ebd9ad4fa21c0d56433ac57a4cd69e94c72281',\n",
       "  '3ca3cbb5a832ddb7fab610501920ab622bd520b0',\n",
       "  0),\n",
       " ('09ebd9ad4fa21c0d56433ac57a4cd69e94c72281',\n",
       "  'cc7fa2cf9d7d2b3aca4fa22271412831e9a61e22',\n",
       "  0),\n",
       " ('09ebd9ad4fa21c0d56433ac57a4cd69e94c72281',\n",
       "  '09ebd9ad4fa21c0d56433ac57a4cd69e94c72281',\n",
       "  0),\n",
       " ('09ebd9ad4fa21c0d56433ac57a4cd69e94c72281',\n",
       "  '590bd948e06e9d07e305fe175c2a86d751ccac2d',\n",
       "  0),\n",
       " ('06b9ed2d9a27d715511a597f7bd5cd88e81f022f',\n",
       "  '57e562b46338f176e3b20c2dd0b66f17dfbef9e8',\n",
       "  0),\n",
       " ('06b9ed2d9a27d715511a597f7bd5cd88e81f022f',\n",
       "  '032fbcb58e2282f02426a0f09c6d5b42787936ec',\n",
       "  1),\n",
       " ('06b9ed2d9a27d715511a597f7bd5cd88e81f022f',\n",
       "  '06b9ed2d9a27d715511a597f7bd5cd88e81f022f',\n",
       "  0),\n",
       " ('06b9ed2d9a27d715511a597f7bd5cd88e81f022f',\n",
       "  'faa5d6d1285eb5f39f28f275cdd2bd8dfbd53a8d',\n",
       "  0),\n",
       " ('06b9ed2d9a27d715511a597f7bd5cd88e81f022f',\n",
       "  '4b762c0344f14bb00d590f5666c27b3aac7b0a7d',\n",
       "  0)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d083506c-227d-482a-bbf9-204aa79291a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/992 [00:00<01:19, 12.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 papers processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 992/992 [01:34<00:00, 10.46it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = PaperPairDataset(val_paper_ids, embedding_map, reference_map, metadata, all_paper_ids, knn=True, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "47b29bc6-85ee-43fa-9a67-26f245017373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the data in order to not have to recompute KNN pairs again\n",
    "with open('prepared_dataset_k100.pkl', 'wb') as f:\n",
    "    pickle.dump(train_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1deccb02-6420-4d5b-bd08-6365cbf3c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prepared_dataset_k100.pkl', 'rb') as f:\n",
    "    temp_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d02abac-3655-48a3-979a-a132343e8ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperPairModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=768, hidden_dim=2048, num_hidden_layers=2, dropout_prob=0.3, weight_decay=0.01):\n",
    "        \n",
    "        super(PaperPairModel, self).__init__()\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        self.fc_layers.append(nn.Linear(embedding_dim * 2, hidden_dim))\n",
    "        for i in range(num_hidden_layers - 1):\n",
    "            self.fc_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        for layer in self.fc_layers:\n",
    "            x = layer(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc_out(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "    def l2_regularization_loss(self):\n",
    "        l2_loss = 0.0\n",
    "        for param in self.parameters():\n",
    "            l2_loss += torch.norm(param, p=2)**2\n",
    "        return self.weight_decay * l2_loss\n",
    "\n",
    "    def loss(self, outputs, targets):\n",
    "        bce_loss = nn.BCELoss()(outputs, targets)\n",
    "        l2_loss = self.l2_regularization_loss()\n",
    "        return bce_loss + l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "10b1dffb-b54b-4780-a76f-3725b5819499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PaperPairModel(nn.Module):\n",
    "#     def __init__(self, embedding_dim=768, hidden_dim=2048, num_hidden_layers=2):\n",
    "        \n",
    "#         super(PaperPairModel, self).__init__()\n",
    "#         self.fc_layers = nn.ModuleList()\n",
    "#         self.fc_layers.append(nn.Linear(embedding_dim * 2, hidden_dim))\n",
    "#         for i in range(num_hidden_layers - 1):\n",
    "#             self.fc_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x1, x2):\n",
    "#         # print(x1.shape, x2. shape)\n",
    "#         x = torch.cat((x1, x2), dim=1)\n",
    "#         for layer in self.fc_layers:\n",
    "#             x = layer(x)\n",
    "#             x = self.relu(x)\n",
    "#         x = self.fc_out(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c45ad5d-292d-45b5-bb9a-72dc2c7e6fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PaperPairModel(nn.Module):\n",
    "#     def __init__(self, embedding_dim=768, hidden_dim=256):\n",
    "        \n",
    "#         super(PaperPairModel, self).__init__()\n",
    "#         self.fc1 = nn.Linear(embedding_dim * 2, hidden_dim)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x1, x2):\n",
    "#         # print(x1.shape, x2. shape)\n",
    "#         x = torch.cat((x1, x2), dim=1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e12a7bc-a4f3-4189-8939-a0a8f3494d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "884f4934-ae72-44ed-ad35-c0944a77be46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59597"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "062e9c3b-3bb4-40b4-992b-b3df2d548f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59597"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "38117b16-ee3e-4527-a80d-a6473bd3bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7973a7ae-3419-4e49-acec-5b0fbe4f83e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PaperPairModel()\n",
    "model.to(DEVICE)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "72f22ecf-895d-4366-9689-6392c35c97ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3a719727-15b7-40ac-a496-bbc5dff654c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2 - NOT THAT GOOD\n",
    "# # Train the model\n",
    "# num_epochs = 100\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for i, data in enumerate(train_dataloader, 0):\n",
    "#         embeddings1, embeddings2, labels = data\n",
    "#         embeddings1 = embeddings1.to(DEVICE)\n",
    "#         embeddings2 = embeddings2.to(DEVICE)\n",
    "#         labels = labels.to(DEVICE)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = model(embeddings1, embeddings2)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7546b8ff-7578-4b35-be46-91be80b42fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 932/932 [00:03<00:00, 253.66batch/s, Train Loss=34]  \n",
      "Epoch 2: 100%|██████████| 932/932 [00:03<00:00, 259.83batch/s, Train Loss=34]  \n",
      "Epoch 3: 100%|██████████| 932/932 [00:03<00:00, 258.84batch/s, Train Loss=34]  \n",
      "Epoch 4: 100%|██████████| 932/932 [00:03<00:00, 252.27batch/s, Train Loss=34]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 932/932 [00:03<00:00, 251.61batch/s, Train Loss=34]  \n",
      "Epoch 6: 100%|██████████| 932/932 [00:03<00:00, 258.35batch/s, Train Loss=34]  \n",
      "Epoch 7: 100%|██████████| 932/932 [00:03<00:00, 261.26batch/s, Train Loss=34]  \n",
      "Epoch 8: 100%|██████████| 932/932 [00:03<00:00, 254.63batch/s, Train Loss=34]  \n",
      "Epoch 9: 100%|██████████| 932/932 [00:03<00:00, 258.45batch/s, Train Loss=34]  \n",
      "Epoch 10: 100%|██████████| 932/932 [00:03<00:00, 248.25batch/s, Train Loss=34]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00052: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    }
   ],
   "source": [
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     # Use tqdm to track the progress of the training loop\n",
    "#     pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\", unit=\"batch\")\n",
    "\n",
    "#     for i, data in enumerate(pbar):\n",
    "#         embeddings1, embeddings2, labels = data\n",
    "#         embeddings1 = embeddings1.to(DEVICE)\n",
    "#         embeddings2 = embeddings2.to(DEVICE)\n",
    "#         labels = labels.to(DEVICE)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = model(embeddings1, embeddings2) # Compute outputs before loss\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#         # Update the progress bar with the current loss\n",
    "#         pbar.set_postfix({\"Train Loss\": running_loss / (i + 1)})\n",
    "        \n",
    "#     epoch_loss = running_loss / len(train_dataloader)\n",
    "    \n",
    "#     # Compute the validation loss\n",
    "#     val_loss = 0.0\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for data in val_dataloader:\n",
    "#             embeddings1, embeddings2, labels = data\n",
    "#             embeddings1 = embeddings1.to(DEVICE)\n",
    "#             embeddings2 = embeddings2.to(DEVICE)\n",
    "#             labels = labels.to(DEVICE)\n",
    "#             outputs = model(embeddings1, embeddings2) # Compute outputs before loss\n",
    "#             val_loss += criterion(outputs, labels).item()\n",
    "#     val_loss /= len(val_dataloader)\n",
    "#     model.train()\n",
    "    \n",
    "#     # Update the progress bar with the train and validation losses\n",
    "#     pbar.set_postfix({\"Train Loss\": epoch_loss, \"Val Loss\": val_loss})\n",
    "#     pbar.close()\n",
    "\n",
    "#     # Update the learning rate scheduler with the validation loss\n",
    "#     lr_scheduler.step(val_loss) # Call it once per epoch with val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ba2701b-0899-4b38-abed-0eb47834663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ORIGINAL WORKING Train the model\n",
    "# num_epochs = 50\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     # Use tqdm to track the progress of the training loop\n",
    "#     pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\", unit=\"batch\")\n",
    "\n",
    "#     for i, data in enumerate(pbar):\n",
    "#         embeddings1, embeddings2, labels = data\n",
    "#         embeddings1 = embeddings1.to(DEVICE)\n",
    "#         embeddings2 = embeddings2.to(DEVICE)\n",
    "#         labels = labels.to(DEVICE)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = model(embeddings1, embeddings2)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#         # Update the progress bar with the current loss\n",
    "#         pbar.set_postfix({\"Loss\": running_loss / (i + 1)})\n",
    "        \n",
    "#     epoch_loss = running_loss / len(train_dataloader)\n",
    "#     lr_scheduler.step(epoch_loss)\n",
    "\n",
    "#     # Update the progress bar with the final loss for the epoch\n",
    "#     pbar.set_postfix({\"Loss\": epoch_loss})\n",
    "#     pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "206e3bb6-875f-491f-ba99-21bb336d9950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the learning rate scheduler with early stopping\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cc97140d-2f51-4f10-9f29-8abdbb4bed3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 932/932 [00:04<00:00, 195.32batch/s, Loss=34]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 932/932 [00:04<00:00, 189.19batch/s, Loss=34]  \n",
      "Epoch 3: 100%|██████████| 932/932 [00:04<00:00, 192.69batch/s, Loss=34]  \n",
      "Epoch 4: 100%|██████████| 932/932 [00:04<00:00, 193.22batch/s, Loss=34]  \n",
      "Epoch 5: 100%|██████████| 932/932 [00:04<00:00, 196.09batch/s, Loss=34]  \n",
      "Epoch 6: 100%|██████████| 932/932 [00:04<00:00, 207.90batch/s, Loss=34]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    # Use tqdm to track the progress of the training loop\n",
    "    pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\", unit=\"batch\")\n",
    "\n",
    "    for i, data in enumerate(pbar):\n",
    "        embeddings1, embeddings2, labels = data\n",
    "        embeddings1 = embeddings1.to(DEVICE)\n",
    "        embeddings2 = embeddings2.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(embeddings1, embeddings2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Update the progress bar with the current loss\n",
    "        pbar.set_postfix({\"Loss\": running_loss / (i + 1)})\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            embeddings1, embeddings2, labels = data\n",
    "            embeddings1 = embeddings1.to(DEVICE)\n",
    "            embeddings2 = embeddings2.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(embeddings1, embeddings2)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "\n",
    "            running_val_loss += val_loss.item()\n",
    "\n",
    "    val_loss = running_val_loss / len(val_dataloader)\n",
    "    model.train()\n",
    "\n",
    "    # Update the learning rate scheduler and check for early stopping\n",
    "    lr_scheduler.step(val_loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        # Save the best model checkpoint\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        # Stop the training if the validation loss does not improve for 5 consecutive epochs\n",
    "        if lr_scheduler.num_bad_epochs == 5:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # Update the progress bar with the final loss for the epoch\n",
    "    pbar.set_postfix({\"Loss\": epoch_loss, \"Val Loss\": val_loss})\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5cfc93-6427-473b-9746-5f57f73dcae1",
   "metadata": {},
   "source": [
    "# OLD TRAINING LOOP\n",
    "Epoch 1, Loss: 0.3235005284722729\n",
    "Epoch 2, Loss: 0.24409154705463235\n",
    "\n",
    "Epoch 99, Loss: 0.013191471295460503\n",
    "Epoch 100, Loss: 0.01251882209242419"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855fd25e-54a4-47ca-9e31-dd5f64790e16",
   "metadata": {},
   "source": [
    "#### Random Intialization\n",
    "\n",
    "Epoch 1, Loss: 0.3234181096924703\n",
    "Epoch 2, Loss: 0.24197020446493894\n",
    "\n",
    "Epoch 9, Loss: 0.12795742934756973\n",
    "Epoch 10, Loss: 0.1209221490487618\n",
    "\n",
    "Epoch 25, Loss: 0.05616574249471324\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbded3cd-a099-45ea-b6d4-834f5485c65f",
   "metadata": {},
   "source": [
    "#### KNN Initializatoin\n",
    "\n",
    "Epoch 1, Loss: 0.33282718187655747\n",
    "Epoch 2, Loss: 0.307354608664441\n",
    "Epoch 3, Loss: 0.2996351488073414\n",
    "\n",
    "Epoch 24, Loss: 0.25681713322160826\n",
    "Epoch 25, Loss: 0.25580120286751623\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e45d9f13-6cb0-470b-8ef3-c9f638dda28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(model, paper_id, embedding_map, k=20, knn_k = 50):\n",
    "    model.eval()\n",
    "    paper_embedding = embedding_map[paper_id]\n",
    "    # other_paper_ids = [pid for pid in embedding_map if pid != paper_id]\n",
    "    # paper_embedding = get_embedding(metadata[paper_id])\n",
    "    top_indices, top_values = find_similar_knn(paper_embedding.view(1,-1), weights, k=knn_k, least=False)\n",
    "    recommended_paper_ids = [all_paper_ids[i] for i in top_indices]\n",
    "    other_paper_embeddings = torch.stack([embedding_map[pid] for pid in recommended_paper_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        paper_embedding = paper_embedding.expand_as(other_paper_embeddings).to(DEVICE)\n",
    "        other_paper_embeddings = other_paper_embeddings.to(DEVICE)\n",
    "        scores = model(paper_embedding, other_paper_embeddings)\n",
    "    model.train()\n",
    "    top_k_indices = torch.topk(scores, k=k).indices\n",
    "    return [recommended_paper_ids[idx] for idx in top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "06f7bcf8-e675-4ce5-8960-ae1c24c26efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paper_id = '204e3073870fae3d05bcbc2f6a8e263d9b72e776'\n",
    "model_rec_ids = get_recommendations(model, test_paper_id, embedding_map, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bf2ae63f-de1c-4962-bbf5-9a9ff1efc03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper ID: 2826f9dccdcceb113b33ccf2841d488f1419bb30\n",
      "Title: Stanford Neural Machine Translation Systems for Spoken Language Domains\n",
      "Year: 2015\n",
      "Paper ID: 98445f4172659ec5e891e031d8202c102135c644\n",
      "Title: Neural Machine Translation in Linear Time\n",
      "Year: 2016\n",
      "Paper ID: 2d876ed1dd2c58058d7197b734a8e4d349b8f231\n",
      "Title: Quasi-Recurrent Neural Networks\n",
      "Year: 2016\n",
      "Paper ID: 93a9694b6a4149e815c30a360347593b75860761\n",
      "Title: Variable-Length Word Encodings for Neural Translation Models\n",
      "Year: 2015\n",
      "Paper ID: 71480da09af638260801af1db8eff6acb4e1122f\n",
      "Title: Decoding with Large-Scale Neural Language Models Improves Translation\n",
      "Year: 2013\n",
      "Paper ID: d9f6ada77448664b71128bb19df15765336974a6\n",
      "Title: SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\n",
      "Year: 2019\n",
      "Paper ID: 654a3e53fb41d8168798ee0ee61dfab73739b1ed\n",
      "Title: Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks\n",
      "Year: 2015\n",
      "Paper ID: 67d968c7450878190e45ac7886746de867bf673d\n",
      "Title: Neural Architecture Search with Reinforcement Learning\n",
      "Year: 2016\n",
      "Paper ID: f958d4921951e394057a1c4ec33bad9a34e5dad1\n",
      "Title: A Convolutional Encoder Model for Neural Machine Translation\n",
      "Year: 2016\n",
      "Paper ID: e75b3c12da067552fda910a5bbed8b4d0e82dbcb\n",
      "Title:  Neural Network Methods for Natural Language Processing\n",
      "Year: 2017\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for paper_id in model_rec_ids:\n",
    "    title = metadata[paper_id]['title']\n",
    "    # abstract = metadata[paper_id]['abstract']\n",
    "    year = metadata[paper_id]['year']\n",
    "    print(f'Paper ID: {paper_id}\\nTitle: {title}\\nYear: {year}')\n",
    "    cnt += 1 \n",
    "    if cnt == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "242c00bf-9dbc-4ae1-ace7-08adf0df4893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, paper_ids, embedding_map, reference_map, k=20, knn_k=50):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    temp_cnt = 0\n",
    "    for paper_id in tqdm(paper_ids):\n",
    "\n",
    "        recommendations = set(get_recommendations(model, paper_id, embedding_map, k=k, knn_k=knn_k))\n",
    "        true_references = set(reference_map.get(paper_id, []))\n",
    "        if not true_references:\n",
    "            continue\n",
    "\n",
    "        intersect = recommendations.intersection(true_references)\n",
    "        precision = len(intersect) / len(recommendations)\n",
    "        recall = len(intersect) / len(true_references)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        temp_cnt += 1\n",
    "        if temp_cnt == 100:\n",
    "            break\n",
    "\n",
    "    mean_precision = sum(precisions) / len(precisions)\n",
    "    mean_recall = sum(recalls) / len(recalls)\n",
    "    return mean_precision, mean_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f904e7ac-ae27-4322-8377-c6f209846cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision @ 20: 0.057399999999999986\n",
      "Recall @ 20: 0.12419555247309377\n"
     ]
    }
   ],
   "source": [
    "precision, recall = evaluate_model(model, test_paper_ids, embedding_map, reference_map, k=50)\n",
    "print(f\"Precision @ 20: {precision}\")\n",
    "print(f\"Recall @ 20: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a95ef4a-1a0b-4569-96d6-20056e085403",
   "metadata": {},
   "source": [
    "### Previous score with 50 epochs\n",
    "Precision @ 20: 0.057399999999999986\n",
    "Recall @ 20: 0.12419555247309377\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0f11e350-a34d-450f-80af-d4808ff24f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.19it/s]\n",
      "100%|██████████| 100/100 [00:10<00:00,  9.58it/s]\n",
      "100%|██████████| 100/100 [00:08<00:00, 11.21it/s]\n",
      "100%|██████████| 100/100 [00:10<00:00,  9.88it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a list of possible values of k\n",
    "k_values = [50, 100, 150, 200]\n",
    "\n",
    "# Initialize empty lists to store the precision and recall scores for each value of k\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "# Evaluate the model for each value of k and append the scores to the lists\n",
    "for k in k_values:\n",
    "    precision, recall = evaluate_model(model, test_paper_ids[:100], embedding_map, reference_map, knn_k=k)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "# Plot the precision and recall curves as a function of k\n",
    "plt.plot(k_values, precisions, label=\"Precision\")\n",
    "plt.plot(k_values, recalls, label=\"Recall\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d87df66-9902-4a09-aa23-b3de41518e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c71acf7-0439-48ac-aa6c-7c6185bafd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4fe6e8-b790-415c-a21b-bfe0ec5dba91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68edba41-48b9-4907-ae48-582d1356e268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32456ff1-c51b-423d-a3f8-e9c48d6e038f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f65348-5ce1-4ea4-8fe1-bceae6cd24c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c03fd-50f9-48f4-9acf-fb79ca91dcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0266422-25dd-428f-b62d-a1c2f612ba19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42251e8c-0b8e-457c-8891-a4fc1f214e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4a241-763e-4064-9f39-10c5ea0ce052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b47682-da18-4a81-8665-4198fdb67b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c735984-49f8-49a7-80df-ae5dea9bc96d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f46e6-b149-470c-9016-a7022c24ba66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d07ecfd-ad5b-49f4-9708-15dbc66fbdee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ead9cb-f9bb-442e-80a9-a034c57a91fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1996be1a-64a3-40d6-98ab-c00014170107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2930c47-3c3c-4464-b34f-54eb2d829d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc3be035-2efc-4dff-85b1-ef233b925206",
   "metadata": {},
   "source": [
    "#### Defining the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "42848951-661f-483f-aa4d-f45143b476eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseRankingLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(PairwiseRankingLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)\n",
    "        loss = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "13bea2dd-d991-49b3-ab4f-3e5e2e6612ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    # get the max sequence length\n",
    "    max_size = max([len(b[1]) for b in batch])\n",
    "    # pad the sequences\n",
    "    padded_batch = []\n",
    "    for b in batch:\n",
    "        padded_paper_embedding = torch.tensor(b[0])\n",
    "        padded_reference_embeddings = pad_sequence(b[1], batch_first=True, padding_value=0)\n",
    "        padded_batch.append((padded_paper_embedding, padded_reference_embeddings))\n",
    "    return padded_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "66ed00e6-89d3-4a48-bae6-cd4ef867dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PaperDataset(embedding_map, reference_map)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)#, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "67ee46fd-0b87-4958-8fa4-f1bd535806ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 768\n",
    "hidden_size = 256\n",
    "margin = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "810586c7-9aa2-4781-ac9a-620acc240e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FineTuningModel(input_size, hidden_size)\n",
    "model.to(DEVICE)\n",
    "loss_function = PairwiseRankingLoss(margin)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e59f255-67e1-4083-b5e1-12659c85696c",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e99677cf-dcc5-41e1-989e-25da6f5778ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62/3324542483.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(paper_embedding, dtype=torch.float32), torch.stack(reference_embeddings)\n",
      "/tmp/ipykernel_62/289649723.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  paper_embedding = torch.tensor(paper_embedding).to(DEVICE)\n",
      "/tmp/ipykernel_62/289649723.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  reference_embeddings = torch.tensor(reference_embeddings).to(DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.0000286102294922\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        paper_embedding, reference_embeddings = batch\n",
    "        # paper_embedding.to(DEVICE)\n",
    "        # reference_embeddings.to(DEVICE)\n",
    "        paper_embedding = torch.tensor(paper_embedding).to(DEVICE)\n",
    "        reference_embeddings = torch.tensor(reference_embeddings).to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Generate negative samples\n",
    "        negative_indices = np.random.choice(len(dataset), len(reference_embeddings))\n",
    "        negative_embeddings = torch.stack([dataset[i][0] for i in negative_indices])\n",
    "        negative_embeddings = negative_embeddings.to(DEVICE)\n",
    "\n",
    "        anchor_output = model(paper_embedding)\n",
    "        positive_outputs = model(reference_embeddings)\n",
    "        negative_outputs = model(negative_embeddings)\n",
    "\n",
    "        loss = loss_function(anchor_output, positive_outputs, negative_outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f090bb2-edd5-4a04-bcf6-7c0b8cf7ba70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34216259-66bb-4abc-8a2e-2b51cbe548cc",
   "metadata": {},
   "source": [
    "#### Evaluation with Precision @ 20 and Recall @ 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5aed7ce8-4533-4cb3-be29-00ea82650199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f4d2938e-a801-481a-bcb6-e450cd5a70c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62/3324542483.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(paper_embedding, dtype=torch.float32), torch.stack(reference_embeddings)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [164], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m         mean_recall \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(recall_scores)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mean_precision, mean_recall\n\u001b[0;32m---> 31\u001b[0m precision, recall \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision@20: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall@20: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [164], line 12\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_dataloader)\u001b[0m\n\u001b[1;32m     10\u001b[0m paper_embedding\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     11\u001b[0m reference_embeddings\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 12\u001b[0m anchor_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m reference_outputs \u001b[38;5;241m=\u001b[39m model(reference_embeddings)\n\u001b[1;32m     15\u001b[0m _, top_k_indices \u001b[38;5;241m=\u001b[39m reference_outputs\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;241m20\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [24], line 9\u001b[0m, in \u001b[0;36mFineTuningModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "\n",
    "        for batch in test_dataloader:\n",
    "            paper_embedding, reference_embeddings = batch\n",
    "            paper_embedding.to(DEVICE)\n",
    "            reference_embeddings.to(DEVICE)\n",
    "            anchor_output = model(paper_embedding)\n",
    "            reference_outputs = model(reference_embeddings)\n",
    "\n",
    "            _, top_k_indices = reference_outputs.topk(20, dim=1)\n",
    "            top_k_references = reference_embeddings[top_k_indices]\n",
    "            top_k_references.to(DEVICE)\n",
    "\n",
    "            true_positive = (top_k_references == reference_embeddings).sum().item()\n",
    "            precision = true_positive / 20\n",
    "            recall = true_positive / len(reference_embeddings)\n",
    "\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "\n",
    "        mean_precision = np.mean(precision_scores)\n",
    "        mean_recall = np.mean(recall_scores)\n",
    "\n",
    "    return mean_precision, mean_recall\n",
    "\n",
    "precision, recall = evaluate_model(model, dataloader)\n",
    "print(f\"Precision@20: {precision:.4f}\")\n",
    "print(f\"Recall@20: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c665612-aaa1-4a2e-aea7-1462093bc0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d98fa08-9867-4eb3-b18c-4fe1bf23f546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14740d64-6b28-4073-8cac-2fb292ec02d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525a5868-3dff-496e-b254-40537850ffa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86668227-2304-45f7-99e5-0b2b94f7f7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47c75444-abf8-4b55-97da-3274502430b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RankNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), dim=0)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "048c4f33-a701-4c14-9b80-be170c7cb1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_ranking_loss(scores, labels, margin=1.0):\n",
    "    diff = labels * (margin - scores)\n",
    "    loss = torch.nn.functional.relu(diff)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e72c7e5a-cdd7-4d8f-b737-1458ee8ed0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 paper IDs processed\n",
      "500 paper IDs processed\n",
      "1000 paper IDs processed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m cur_paper \u001b[38;5;241m=\u001b[39m metadata[cur_paper_id]\n\u001b[1;32m     12\u001b[0m cur_paper_emb \u001b[38;5;241m=\u001b[39m get_embedding(cur_paper, embedding_map \u001b[38;5;241m=\u001b[39m embedding_map)\n\u001b[0;32m---> 13\u001b[0m top_indices, top_values \u001b[38;5;241m=\u001b[39m \u001b[43mfind_similar_knn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_paper_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_recommendations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m cur_recommendations \u001b[38;5;241m=\u001b[39m [all_paper_ids[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m top_indices]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# TODO remove this line \u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [14], line 7\u001b[0m, in \u001b[0;36mfind_similar_knn\u001b[0;34m(cur_embedding, weights, k, least)\u001b[0m\n\u001b[1;32m      5\u001b[0m weights_norm \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdouble() \u001b[38;5;66;03m# (N, d)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m cur_em_norm \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(cur_embedding, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdouble() \u001b[38;5;66;03m# (1, d)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m cos_sim \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m topk \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(cos_sim, k, largest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m least \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m top_indices \u001b[38;5;241m=\u001b[39m topk\u001b[38;5;241m.\u001b[39mindices\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_recommendations = 100\n",
    "K = 10\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# TODO: optimize with multiprocessing with 8 processes and balance -1s with 1s (say 10 of each per paper)\n",
    "\n",
    "for i, cur_paper_id in enumerate(all_paper_ids):\n",
    "    cur_references = reference_map[cur_paper_id]\n",
    "    cur_paper = metadata[cur_paper_id]\n",
    "    cur_paper_emb = get_embedding(cur_paper, embedding_map = embedding_map)\n",
    "    top_indices, top_values = find_similar_knn(cur_paper_emb, weights, k=num_recommendations, least=False)\n",
    "    cur_recommendations = [all_paper_ids[i] for i in top_indices]\n",
    "\n",
    "    # TODO remove this line \n",
    "    cur_recommendations.remove(cur_paper_id)\n",
    "    \n",
    "    # p,r,f1 = evaluate(cur_recommendations, cur_references, k=K)\n",
    "    # print(f'{cur_paper_id=}{p=}{r=}{f1=}')\n",
    "    \n",
    "    for rec in cur_recommendations:\n",
    "        X.append((cur_paper_id, rec))\n",
    "        y.append(1 if rec in cur_references else -1)\n",
    "        \n",
    "    if i % 500 == 0: \n",
    "        print(f'{i} paper IDs processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b47b83f-c97b-4313-a63e-264f9777c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sanity_check(X=None, y = None, sample_size = 20):\n",
    "    assert(X is not None and y is not None)\n",
    "    # randomly sample 5 items from both X and y\n",
    "    indices = random.sample(range(len(X)), sample_size)\n",
    "    X_sample = [X[i] for i in indices]\n",
    "    y_sample = [y[i] for i in indices]\n",
    "\n",
    "    # print the sampled X and y\n",
    "    # print(X_sample)  # output: [4, 8, 2, 9, 1]\n",
    "    # print(y_sample)  # output: ['d', 'i', 'b', 'j', 'a']\n",
    "\n",
    "    for (q,r), label in zip(X_sample, y_sample):\n",
    "        q_refs = reference_map[q]\n",
    "        \n",
    "        if (label == 1 and r not in q_refs) or (label == -1 and r in q_refs):\n",
    "            print('There\\'s been a problem')\n",
    "            return\n",
    "            \n",
    "    print('Saul Goodman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ae938f7-dded-4af9-ab40-08d68394ee22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saul Goodman\n"
     ]
    }
   ],
   "source": [
    "data_sanity_check(X, y, sample_size = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd43e4ba-e636-4594-9cf7-ba14e2e1870e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131571, 131571)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "344c2bd9-d323-4b6c-9d9f-f7cd6c34fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 768\n",
    "input_size = embedding_dim * 2\n",
    "hidden_size = 256\n",
    "output_size = 1\n",
    "model = RankNet(input_size, hidden_size, output_size).to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss = torch.nn.MarginRankingLoss(margin = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4a0a28a-0c1b-4369-9fce-f7e9a68e9bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([y[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7878339e-28b9-4d28-b9c5-16dba59e1768",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlabel\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label' is not defined"
     ]
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bccad35b-5da6-4c01-8d76-41e9114b397c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(score1, score2, label).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eeb334c4-8573-4819-bed7-fab9dd6c3c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed50c0f0-195c-480b-a56d-d7213d4f345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: loss=MarginRankingLoss()\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    for i, (cur_id, rec_id) in enumerate(X):\n",
    "        x1 = torch.tensor(embedding_map[cur_id], dtype=torch.float32, requires_grad=True).to(DEVICE)\n",
    "        x2 = torch.tensor(embedding_map[rec_id], dtype=torch.float32, requires_grad=True).to(DEVICE)\n",
    "        label = torch.tensor([y[i]], dtype=torch.float32, requires_grad=True).to(DEVICE)\n",
    "        score1 = model(x1, x2)\n",
    "        score2 = model(x2, x1)\n",
    "        # loss = pairwise_ranking_loss(score1 - score2, y[i])\n",
    "        # x1o, x2o = out.chunk(2)\n",
    "        output = loss(score1, score2, label)\n",
    "        optimizer.zero_grad()\n",
    "        output.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            \n",
    "            print(output.item())\n",
    "        \n",
    "        \n",
    "    print(f'{epoch=}: {loss=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b8350ae-d74a-4bff-bc6f-b27cb8652286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(embedding_map[rec_id]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9edf9cf6-730f-48bb-8272-86edeb7f399d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31525631-3af9-41f5-aa58-e6bfac516c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7745bb8-2711-4b39-801f-99e9e1c4b4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00023286789655685425\n",
      "-0.00023286789655685425\n",
      "-0.00023286789655685425\n",
      "-0.00023286789655685425\n",
      "-0.00023286789655685425\n",
      "-0.00023286789655685425\n",
      "-0.00023286789655685425\n",
      "-0.00023286789655685425\n",
      "-0.00023286789655685425\n",
      "-0.00023286789655685425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RankNet(\n",
       "  (fc1): Linear(in_features=1536, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_paper = {'title': new_title, 'abstract': new_abstract or ''}\n",
    "# assert new_paper['title'] is not None\n",
    "# paper_id = '204e3073870fae3d05bcbc2f6a8e263d9b72e776'\n",
    "k = 10\n",
    "paper_id = '204e3073870fae3d05bcbc2f6a8e263d9b72e776'\n",
    "paper_embedding = get_embedding(metadata[paper_id])\n",
    "top_indices, top_values = find_similar_knn(paper_embedding, weights, k=10, least=False)\n",
    "recommended_paper_ids = [all_paper_ids[i] for i in top_indices]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = []\n",
    "    x1 = paper_embedding.view(-1).float().to(DEVICE)\n",
    "    \n",
    "    for idx, rec_id in enumerate(recommended_paper_ids):\n",
    "        # (768,) -> (1, 768)\n",
    "        x2 = torch.tensor(embedding_map[rec_id]).float().to(DEVICE)\n",
    "        # print(x1.shape)\n",
    "        score = model(x1, x2).item()\n",
    "        print(score)\n",
    "        scores.append((score, idx))\n",
    "        idx += 1\n",
    "        \n",
    "    scores.sort(reverse=True)\n",
    "    updated_rec_ids = [(recommended_paper_ids[j], score) for score, j in scores]\n",
    "    \n",
    "model.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "950e238e-7980-4489-8fed-7689e6e99ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.00023286789655685425, 9),\n",
       " (-0.00023286789655685425, 8),\n",
       " (-0.00023286789655685425, 7),\n",
       " (-0.00023286789655685425, 6),\n",
       " (-0.00023286789655685425, 5),\n",
       " (-0.00023286789655685425, 4),\n",
       " (-0.00023286789655685425, 3),\n",
       " (-0.00023286789655685425, 2),\n",
       " (-0.00023286789655685425, 1),\n",
       " (-0.00023286789655685425, 0)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ebbd2062-0b27-49cd-9c62-1e3cbf4f8b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bf8fe437f779f2098f9af82b534aa51dc9edb06f', -0.00023286789655685425),\n",
       " ('25eb839f39507fe6983ad3e692b2f8d93a5cb0cc', -0.00023286789655685425),\n",
       " ('bb669de2fce407df2f5cb2f8c51dedee3f467e04', -0.00023286789655685425),\n",
       " ('4550a4c714920ef57d19878e31c9ebae37b049b2', -0.00023286789655685425),\n",
       " ('93499a7c7f699b6630a86fad964536f9423bb6d0', -0.00023286789655685425),\n",
       " ('43428880d75b3a14257c3ee9bda054e61eb869c0', -0.00023286789655685425),\n",
       " ('dbde7dfa6cae81df8ac19ef500c42db96c3d1edd', -0.00023286789655685425),\n",
       " ('9ae0a24f0928cab1554a6ac880f6b350f85be698', -0.00023286789655685425),\n",
       " ('b60abe57bc195616063be10638c6437358c81d1e', -0.00023286789655685425),\n",
       " ('204e3073870fae3d05bcbc2f6a8e263d9b72e776', -0.00023286789655685425)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_rec_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9ceedea-5ecc-404c-a30a-6643f5ef082d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper ID: bf8fe437f779f2098f9af82b534aa51dc9edb06f\n",
      "Title: Scaling Neural Machine Translation\n",
      "Year: 2018\n",
      " Updated similarity: -0.00023286789655685425\n",
      "\n",
      "Paper ID: 25eb839f39507fe6983ad3e692b2f8d93a5cb0cc\n",
      "Title: Montreal Neural Machine Translation Systems for WMT’15\n",
      "Year: 2015\n",
      " Updated similarity: -0.00023286789655685425\n",
      "\n",
      "Paper ID: bb669de2fce407df2f5cb2f8c51dedee3f467e04\n",
      "Title: The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation\n",
      "Year: 2018\n",
      " Updated similarity: -0.00023286789655685425\n",
      "\n",
      "Paper ID: 4550a4c714920ef57d19878e31c9ebae37b049b2\n",
      "Title: Massive Exploration of Neural Machine Translation Architectures\n",
      "Year: 2017\n",
      " Updated similarity: -0.00023286789655685425\n",
      "\n",
      "Paper ID: 93499a7c7f699b6630a86fad964536f9423bb6d0\n",
      "Title: Effective Approaches to Attention-based Neural Machine Translation\n",
      "Year: 2015\n",
      " Updated similarity: -0.00023286789655685425\n",
      "\n",
      "Paper ID: 43428880d75b3a14257c3ee9bda054e61eb869c0\n",
      "Title: Convolutional Sequence to Sequence Learning\n",
      "Year: 2017\n",
      " Updated similarity: -0.00023286789655685425\n",
      "\n",
      "Paper ID: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd\n",
      "Title: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\n",
      "Year: 2016\n",
      " Updated similarity: -0.00023286789655685425\n",
      "\n",
      "Paper ID: 9ae0a24f0928cab1554a6ac880f6b350f85be698\n",
      "Title: One Model To Learn Them All\n",
      "Year: 2017\n",
      " Updated similarity: -0.00023286789655685425\n",
      "\n",
      "Paper ID: b60abe57bc195616063be10638c6437358c81d1e\n",
      "Title: Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation\n",
      "Year: 2016\n",
      " Updated similarity: -0.00023286789655685425\n",
      "\n",
      "Paper ID: 204e3073870fae3d05bcbc2f6a8e263d9b72e776\n",
      "Title: Attention is All you Need\n",
      "Year: 2017\n",
      " Updated similarity: -0.00023286789655685425\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Precision @ 10: 0.36363636363636365\n",
      "Recall @ 10: 0.10810810810810811\n",
      "F1 Score: 0.16666666666666669\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for paper_id, upd_sim in updated_rec_ids:\n",
    "    title = metadata[paper_id]['title']\n",
    "    # abstract = metadata[paper_id]['abstract']\n",
    "    year = metadata[paper_id]['year']\n",
    "    print(f'Paper ID: {paper_id}\\nTitle: {title}\\nYear: {year}\\n Updated similarity: {upd_sim}\\n')\n",
    "    cnt += 1 \n",
    "    if cnt == 10:\n",
    "        break\n",
    "        \n",
    "actual_references = ast.literal_eval(metadata['204e3073870fae3d05bcbc2f6a8e263d9b72e776'].get('references'))\n",
    "\n",
    "print('-'*100)\n",
    "\n",
    "precision, recall, f1_score = evaluate(updated_rec_ids, actual_references, k=k)\n",
    "print(f\"Precision @ {k}: {precision}\")\n",
    "print(f\"Recall @ {k}: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "206457ab4910b064b67455bd75e50e17d8a95a06b82e237351ff89b18574f0fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
