Paper #1:
Research Paper Recommendation System Using Natural Language Processing

The literature review is an essential part of the research process, as it helps researchers understand the current state of knowledge in their field and identify gaps that their research can address. However, the current review process with manual paper searching, can be time-consuming and labour-intensive. This is particularly true for researchers working in fields with large and rapidly- growing bodies of literature such as Medicine or Generative AI. To tackle this issue, we aim to build RefPred - a system that uses a citation-informed transformer (SPECTER) with a recommendation engine to recommend relevant papers to assist researchers in the review process. Specifically, given a new title/abstract, it should be able to predict the most relevant papers and sort them according to some metric (such as citation count or similarity score). For doing this, we create a dataset comprising thousands of research paper metadata, sourced from Semantic Scholar (S2), by crawling from the S2 API asynchronously and storing locally on a MongoDB database. We then use the citation-informed transformer model SPECTER to embed each paper, capturing its citation and semantic meaning simultaneously. Using this, we construct an embedding space of papers, which is used to build a recommendation engine based on KNN as a baseline to give relevant recommendations for a new paper. Finally, we propose a novel approach to use a feed- forward neural network to rerank the initial KNN candidates, resulting in 70% better Precision and Recall @ 20 scores on the test set over the baseline KNN approach.



Paper #2:
Title: Attention Is All You Need
Abstract: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.




PAPER #3
Title: Image quality assessment: from error visibility to structural similarity

Abstract: Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.